{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "\"\"\"\n",
    "Provides scaled and split aggregated customer session data\n",
    "\n",
    ":features: list of features to include\n",
    ":predict: value to predict: 'churned' or 'deltaNextHours'\n",
    "\"\"\"\n",
    "features=None\n",
    "predict='churned'\n",
    "\n",
    "\n",
    "df = pd.read_pickle('../../data/churn/churn.pkl')\n",
    "pred_col = predict\n",
    "\n",
    "if features is None:\n",
    "   features = list(set(df.columns) - set(['customerId','churned','deltaNextHours']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2963\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-31-98f3816a37b5>\"\u001b[0m, line \u001b[0;32m7\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    dmatrices('churned~' + 'deltaNextHours+' + '+'.join(features) + '-1', df)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\highlevel.py\"\u001b[0m, line \u001b[0;32m310\u001b[0m, in \u001b[0;35mdmatrices\u001b[0m\n    NA_action, return_type)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\highlevel.py\"\u001b[0m, line \u001b[0;32m165\u001b[0m, in \u001b[0;35m_do_highlevel_design\u001b[0m\n    NA_action)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\highlevel.py\"\u001b[0m, line \u001b[0;32m70\u001b[0m, in \u001b[0;35m_try_incr_builders\u001b[0m\n    NA_action)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\build.py\"\u001b[0m, line \u001b[0;32m689\u001b[0m, in \u001b[0;35mdesign_matrix_builders\u001b[0m\n    factor_states = _factors_memorize(all_factors, data_iter_maker, eval_env)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\build.py\"\u001b[0m, line \u001b[0;32m354\u001b[0m, in \u001b[0;35m_factors_memorize\u001b[0m\n    which_pass = factor.memorize_passes_needed(state, eval_env)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\eval.py\"\u001b[0m, line \u001b[0;32m474\u001b[0m, in \u001b[0;35mmemorize_passes_needed\u001b[0m\n    subset_names = [name for name in ast_names(self.code)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\eval.py\"\u001b[0m, line \u001b[0;32m474\u001b[0m, in \u001b[0;35m<listcomp>\u001b[0m\n    subset_names = [name for name in ast_names(self.code)\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\patsy\\eval.py\"\u001b[0m, line \u001b[0;32m105\u001b[0m, in \u001b[0;35mast_names\u001b[0m\n    for node in ast.walk(ast.parse(code)):\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\ProgramData\\Anaconda3\\lib\\ast.py\"\u001b[1;36m, line \u001b[1;32m35\u001b[1;36m, in \u001b[1;35mparse\u001b[1;36m\u001b[0m\n\u001b[1;33m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<unknown>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sessionDevice.mobile.\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Select subset of features from original dataset. Re-creates all X/y sets accordingly\n",
    "\"\"\"\n",
    "features = np.array(features)\n",
    "\n",
    "# set deltaNextHours as first column in X\n",
    "#dmatrices('churned~' + 'deltaNextHours+' + '+'.join(features) + '-1', df)\n",
    "\n",
    "#dmatrices('frequency', df)\n",
    "pd.read_csv('../../data/churn/churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-cb2f1dd1dd65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# workaround for missing pickling support in patsy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "        \n",
    "\n",
    "\n",
    "        # workaround for missing pickling support in patsy\n",
    "        self.X = np.array(self.X.tolist())\n",
    "        self.y = np.array(self.y.tolist())\n",
    "\n",
    "        # use stratified split\n",
    "        trainI, testI  = next(StratifiedShuffleSplit(test_size=.2, random_state=42).split(self.X, self.y))\n",
    "        self.X_train0, self.X_test0, self.y_train, self.y_test = self.X[trainI], self.X[testI], self.y[trainI], self.y[testI]\n",
    "\n",
    "        # scale values\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train = self.scaler.fit_transform(self.X_train0)\n",
    "        self.X_test = self.scaler.transform(self.X_test0)\n",
    "\n",
    "        # un-scale deltaNextHours column\n",
    "        self.X_train.T[0] = self.X_train0.T[0]\n",
    "        self.X_test.T[0] = self.X_test0.T[0]\n",
    "\n",
    "        # further split training set into train and validation sets\n",
    "        # use stratified split\n",
    "        trainI, testI  = next(StratifiedShuffleSplit(test_size=.2, random_state=42).split(self.X_train, self.y_train))\n",
    "        self.X_split_train, self.X_split_val, self.y_split_train, self.y_split_val = self.X_train[trainI], self.X_train[testI], self.y_train[trainI], self.y_train[testI]\n",
    "\n",
    "        # set features for churned / deltaNextHours prediction\n",
    "        if self.pred_col=='churned':\n",
    "            # remove deltaNextHours and observed columns\n",
    "            self.X = self.X.T[1:].T\n",
    "            self.X_train = self.X_train.T[1:].T\n",
    "            self.X_test = self.X_test.T[1:].T\n",
    "            self.X_split_train = self.X_split_train.T[1:].T\n",
    "            self.X_split_val = self.X_split_val.T[1:].T\n",
    "        elif self.pred_col=='deltaNextHours':\n",
    "            # set y as observed column in X\n",
    "            self.X = np.append(self.X.T, self.y).reshape(-1, len(self.y)).T\n",
    "            self.X_train = np.append(self.X_train.T, self.y_train).reshape(-1, len(self.y_train)).T\n",
    "            self.X_test = np.append(self.X_test.T, self.y_test).reshape(-1, len(self.y_test)).T\n",
    "            self.X_split_train = np.append(self.X_split_train.T, self.y_split_train).reshape(-1, len(self.y_split_train)).T\n",
    "            self.X_split_val = np.append(self.X_split_val.T, self.y_split_val).reshape(-1, len(self.y_split_val)).T\n",
    "            # set deltaNextHours as y\n",
    "            self.y = self.X.T[0].reshape(-1)\n",
    "            self.y_train = self.X_train.T[0].reshape(-1)\n",
    "            self.y_test = self.X_test.T[0].reshape(-1)\n",
    "            self.y_split_train = self.X_split_train.T[0].reshape(-1)\n",
    "            self.y_split_val = self.X_split_val.T[0].reshape(-1)\n",
    "            # remove deltaNextHours from X\n",
    "            self.X = self.X.T[1:].T\n",
    "            self.X_train = self.X_train.T[1:].T\n",
    "            self.X_test = self.X_test.T[1:].T\n",
    "            self.X_split_train = self.X_split_train.T[1:].T\n",
    "            self.X_split_val = self.X_split_val.T[1:].T\n",
    "\n",
    "        self.y_train = self.y_train.reshape(-1)\n",
    "        self.y_test = self.y_test.reshape(-1)\n",
    "        self.y_split_train = self.y_split_train.reshape(-1)\n",
    "        self.y_split_val = self.y_split_val.reshape(-1)\n",
    "\n",
    "        self.train = {'X': self.X_train, 'y': self.y_train}\n",
    "        self.train_df = self._asDf(**self.train)\n",
    "        self.test = {'X': self.X_test, 'y': self.y_test}\n",
    "        self.test_df = self._asDf(**self.test)\n",
    "        self.split_train = {'X': self.X_split_train, 'y': self.y_split_train}\n",
    "        self.split_train_df = self._asDf(**self.split_train)\n",
    "        self.split_val = {'X': self.X_split_val, 'y': self.y_split_val}\n",
    "        self.split_val_df = self._asDf(**self.split_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def setFeatures(self, features):\n",
    "        \"\"\"\n",
    "        Select subset of features from original dataset. Re-creates all X/y sets accordingly\n",
    "        \"\"\"\n",
    "        self.features = np.array(features)\n",
    "\n",
    "        # set deltaNextHours as first column in X\n",
    "        self.y, self.X = dmatrices(\n",
    "                'churned~' + 'deltaNextHours+' + '+'.join(self.features) + '-1',\n",
    "                self.df)\n",
    "\n",
    "        # workaround for missing pickling support in patsy\n",
    "        self.X = np.array(self.X.tolist())\n",
    "        self.y = np.array(self.y.tolist())\n",
    "\n",
    "        # use stratified split\n",
    "        trainI, testI  = next(StratifiedShuffleSplit(test_size=.2, random_state=42).split(self.X, self.y))\n",
    "        self.X_train0, self.X_test0, self.y_train, self.y_test = self.X[trainI], self.X[testI], self.y[trainI], self.y[testI]\n",
    "\n",
    "        # scale values\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train = self.scaler.fit_transform(self.X_train0)\n",
    "        self.X_test = self.scaler.transform(self.X_test0)\n",
    "\n",
    "        # un-scale deltaNextHours column\n",
    "        self.X_train.T[0] = self.X_train0.T[0]\n",
    "        self.X_test.T[0] = self.X_test0.T[0]\n",
    "\n",
    "        # further split training set into train and validation sets\n",
    "        # use stratified split\n",
    "        trainI, testI  = next(StratifiedShuffleSplit(test_size=.2, random_state=42).split(self.X_train, self.y_train))\n",
    "        self.X_split_train, self.X_split_val, self.y_split_train, self.y_split_val = self.X_train[trainI], self.X_train[testI], self.y_train[trainI], self.y_train[testI]\n",
    "\n",
    "        # set features for churned / deltaNextHours prediction\n",
    "        if self.pred_col=='churned':\n",
    "            # remove deltaNextHours and observed columns\n",
    "            self.X = self.X.T[1:].T\n",
    "            self.X_train = self.X_train.T[1:].T\n",
    "            self.X_test = self.X_test.T[1:].T\n",
    "            self.X_split_train = self.X_split_train.T[1:].T\n",
    "            self.X_split_val = self.X_split_val.T[1:].T\n",
    "        elif self.pred_col=='deltaNextHours':\n",
    "            # set y as observed column in X\n",
    "            self.X = np.append(self.X.T, self.y).reshape(-1, len(self.y)).T\n",
    "            self.X_train = np.append(self.X_train.T, self.y_train).reshape(-1, len(self.y_train)).T\n",
    "            self.X_test = np.append(self.X_test.T, self.y_test).reshape(-1, len(self.y_test)).T\n",
    "            self.X_split_train = np.append(self.X_split_train.T, self.y_split_train).reshape(-1, len(self.y_split_train)).T\n",
    "            self.X_split_val = np.append(self.X_split_val.T, self.y_split_val).reshape(-1, len(self.y_split_val)).T\n",
    "            # set deltaNextHours as y\n",
    "            self.y = self.X.T[0].reshape(-1)\n",
    "            self.y_train = self.X_train.T[0].reshape(-1)\n",
    "            self.y_test = self.X_test.T[0].reshape(-1)\n",
    "            self.y_split_train = self.X_split_train.T[0].reshape(-1)\n",
    "            self.y_split_val = self.X_split_val.T[0].reshape(-1)\n",
    "            # remove deltaNextHours from X\n",
    "            self.X = self.X.T[1:].T\n",
    "            self.X_train = self.X_train.T[1:].T\n",
    "            self.X_test = self.X_test.T[1:].T\n",
    "            self.X_split_train = self.X_split_train.T[1:].T\n",
    "            self.X_split_val = self.X_split_val.T[1:].T\n",
    "\n",
    "        self.y_train = self.y_train.reshape(-1)\n",
    "        self.y_test = self.y_test.reshape(-1)\n",
    "        self.y_split_train = self.y_split_train.reshape(-1)\n",
    "        self.y_split_val = self.y_split_val.reshape(-1)\n",
    "\n",
    "        self.train = {'X': self.X_train, 'y': self.y_train}\n",
    "        self.train_df = self._asDf(**self.train)\n",
    "        self.test = {'X': self.X_test, 'y': self.y_test}\n",
    "        self.test_df = self._asDf(**self.test)\n",
    "        self.split_train = {'X': self.X_split_train, 'y': self.y_split_train}\n",
    "        self.split_train_df = self._asDf(**self.split_train)\n",
    "        self.split_val = {'X': self.X_split_val, 'y': self.y_split_val}\n",
    "        self.split_val_df = self._asDf(**self.split_val)\n",
    "\n",
    "\n",
    "    def _asDf(self,X,y):\n",
    "        df = pd.DataFrame(X)\n",
    "        if self.pred_col=='deltaNextHours':\n",
    "            df.columns = self.features.tolist() + ['observed']\n",
    "        else:\n",
    "            df.columns = self.features\n",
    "\n",
    "        df[self.pred_col] = y\n",
    "        return df\n",
    "\n",
    "\n",
    "    def getScores(self, model, dataset='test', X=None, y=None):\n",
    "        if dataset=='test':\n",
    "            X = self.X_test if X is None else X\n",
    "            y = self.y_test if y is None else y\n",
    "        elif dataset=='train':\n",
    "            X = self.X_train if X is None else X\n",
    "            y = self.y_train if y is None else y\n",
    "        elif dataset=='split_train':\n",
    "            X = self.X_split_train if X is None else X\n",
    "            y = self.y_split_train if y is None else y\n",
    "        elif dataset=='split_val':\n",
    "            X = self.X_split_val if X is None else X\n",
    "            y = self.y_split_val if y is None else y\n",
    "\n",
    "        predicted = model.predict(X)\n",
    "        return predicted\n",
    "        probs = model.predict_proba(X)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y, predicted)\n",
    "        auc = metrics.roc_auc_score(y, probs[:, 1])\n",
    "        f1 = metrics.f1_score(y, predicted)\n",
    "        report = metrics.classification_report(y, predicted)\n",
    "        confusion_matrix =  metrics.confusion_matrix(y, predicted)\n",
    "\n",
    "        return {'accuracy': accuracy,\n",
    "                'auc': auc,\n",
    "                'f1': f1,\n",
    "                'classification_report': report,\n",
    "                'confusion_matrix': confusion_matrix}\n",
    "\n",
    "\n",
    "    def printScores(self, model, dataset='test', X=None, y=None):\n",
    "        scores = self.getScores(model, dataset, X, y)\n",
    "        print('Accuracy: {}\\n'.format(scores['accuracy']))\n",
    "        print('AUC: {}\\n'.format(scores['auc']))\n",
    "        print(scores['classification_report'])\n",
    "        print('Confusion matrix:\\n', scores['confusion_matrix'])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
