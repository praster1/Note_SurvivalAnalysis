{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-21e276b2b3fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn_pandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrameMapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'sklearn_pandas'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../utils')\n",
    "from dataPiping import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp, fabs, sqrt, log, pi\n",
    "\n",
    "from random import random\n",
    "import datetime\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import Callback, LambdaCallback, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn_pandas import DataFrameMapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InfluenceLayer(Layer):\n",
    "    \"\"\" RMTPP Influence layer:\n",
    "    Computes fixed terms of RMTPP loss function (negative log-likelihood)\n",
    "    \n",
    "        negative log-likelihood term:\n",
    "\n",
    "        -vt.T*hj - wt(t - tj) - bt - 1/w exp(vt.T*hj + bt) + 1/w exp(vt.T*hj + wt(t-tj) + bt)\n",
    "\n",
    "        vt.T*hj: output of previous layer (past influence)\n",
    "        wt: multiplier on current influence (t-tj)\n",
    "        bt: base intensity\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.dot(x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _neg_log_likelihood(timings, acc_influence):\n",
    "    \"\"\" Loss function for RMTPP model\n",
    "\n",
    "    :timings: vector (t_j, t_(j+1))\n",
    "    :acc_influence: rnn output = v_t * h_j\n",
    "    \"\"\"\n",
    "#     return K.mean(K.square(timings - acc_influence), axis=-1)\n",
    "    wt = -.01\n",
    "    w = .01\n",
    "    bt = .1\n",
    "    \n",
    "    t = timings[0]\n",
    "    tj = timings[1]\n",
    "    return -acc_influence - wt*(t - tj) \\\n",
    "           - bt - 1/w*K.exp(acc_influence + bt) \\\n",
    "           + 1/w*K.exp(acc_influence + wt*(t - tj) + bt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel(lr=100):\n",
    "    in_neurons = 1\n",
    "    out_neurons = 1\n",
    "    input_hidden_neurons = 16\n",
    "    lstm_hidden_neurons = 16\n",
    "    output_hidden_neurons = 1\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # # input layer (weights W_t)\n",
    "    # model.add(Dense(\n",
    "    #     input_hidden_neurons,\n",
    "    #     input_shape=(None, in_neurons),\n",
    "    #     activation='linear'))\n",
    "\n",
    "    # recurrent layer (weights W_h)\n",
    "    model.add(LSTM(\n",
    "        lstm_hidden_neurons,\n",
    "        return_sequences=False,\n",
    "        input_shape=(None, in_neurons), activation='relu'))\n",
    "\n",
    "    # output layer (weights v_t)\n",
    "    model.add(Dense(\n",
    "        out_neurons,\n",
    "        activation='linear'))\n",
    "\n",
    "    # model.compile(loss='mse', optimizer=Adam(lr=lr))\n",
    "    model.compile(loss=_neg_log_likelihood, optimizer=Adam(lr=lr))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getModel()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
