{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dateutil import parser\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeunixtime(val):\n",
    "    try:\n",
    "        return int(time.mktime(parser.parse(val).timetuple()))\n",
    "    except (OverflowError, AttributeError, ValueError):\n",
    "        return None\n",
    "\n",
    "def unixtimetostr(val):\n",
    "    return datetime.datetime.fromtimestamp(int(val)).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _discardNanStartUserTime(df,targets):\n",
    "    nanStartUserTimeCust = df[df.startUserTime.isnull()].customerId.unique()\n",
    "    return df[~df.customerId.isin(nanStartUserTimeCust)], targets[~targets.index.isin(nanStartUserTimeCust)]\n",
    "\n",
    "def _calcStartUserTimeNan(g):\n",
    "    isNan = g.startUserTime.isnull()\n",
    "    \n",
    "    if isNan.all():\n",
    "        return g\n",
    "    \n",
    "    if isNan.any():\n",
    "        timeOffset = g[~isNan].iloc[0].startUserTime - g[~isNan].iloc[0].startTime\n",
    "        for i, row in g[isNan].iterrows():\n",
    "            g.set_value(i, 'startUserTime', row.startTime + timeOffset)\n",
    "        \n",
    "    return g\n",
    "\n",
    "def replaceNanStartUserTime(df, targets):\n",
    "    df = df.groupby('customerId').apply(_calcStartUserTimeNan)\n",
    "    return _discardNanStartUserTime(df, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normaliseData(df,targets):\n",
    "    trainPeriod = [\"2015-02-01\", \"2016-02-01\"]\n",
    "    trainDF = df.copy()\n",
    "    trainTargets = targets.copy()\n",
    "    \n",
    "    trainDF, trainTargets = replaceNanStartUserTime(trainDF, trainTargets)\n",
    "    \n",
    "    trainDF['startTime'] = trainDF['startTime'] - makeunixtime(trainPeriod[0])\n",
    "    trainDF['startUserTime'] = (trainDF['startUserTime'] - makeunixtime(trainPeriod[0])).astype(int)\n",
    "    \n",
    "    uniqueCustomerId = trainDF.customerId.unique()\n",
    "    customerIdEnc = preprocessing.LabelEncoder().fit(uniqueCustomerId)\n",
    "    \n",
    "    trainDF['customerId'] = customerIdEnc.transform(trainDF.customerId)\n",
    "    trainTargets.index = customerIdEnc.transform(trainTargets.index)\n",
    "    \n",
    "    return trainDF, trainTargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainTestSplitCust(X, y, test_size=0.33, random_state=42):\n",
    "    train_cust, test_cust, y_train, y_test = X_test_split(y.keys(), y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    X_train = X[X.customerId.isin(train_cust)]\n",
    "    X_test = X[X.customerId.isin(test_cust)]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMergedSessionData():\n",
    "    trainDF = pd.read_pickle('../../data/trainFebToFebCensoredMergedDF.pkl')\n",
    "    targetDF = pd.read_pickle('../../data/trainFebToFebTargetsMergedDF.pkl')\n",
    "    \n",
    "    trainDF.reset_index(inplace=True)\n",
    "    del trainDF['index']\n",
    "    del trainDF['sessionId']\n",
    "    \n",
    "    targetSeq = targetDF.returnTime\n",
    "    \n",
    "    train, targets = normaliseData(trainDF, targetSeq)\n",
    "    \n",
    "    return train, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Aggregate users into categories listed below (use via aggrUsers)\n",
    "'''\n",
    "def _aggrUser(user):\n",
    "    nSessions = len(user)\n",
    "    period = user.startTime.values[-1] - user.startTime.values[0]\n",
    "    frequency = 0\n",
    "    if period > 0:\n",
    "        frequency = nSessions/period\n",
    "    recency = user.returnTime.values[-1]\n",
    "    avgViewOnly = user.viewonly.sum()/nSessions\n",
    "    avgChangeThumbnail = user.changeThumbnail.sum()/nSessions\n",
    "    avgImageZoom = user.imageZoom.sum()/nSessions\n",
    "    avgWatchVideo = user.watchVideo.sum()/nSessions\n",
    "    avgView360 = user.view360.sum()/nSessions\n",
    "    mobile = (user.device == 'mobile').sum()/nSessions\n",
    "    desktop = (user.device == 'desktop').sum()/nSessions\n",
    "    android = (user.device == 'android').sum()/nSessions\n",
    "    ios = (user.device == 'ios').sum()/nSessions\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'nSessions': nSessions, \n",
    "        'period': period,\n",
    "        'frequency': frequency,\n",
    "        'recency': recency,\n",
    "        'avgViewOnly': avgViewOnly,\n",
    "        'avgChangeThumbnail': avgChangeThumbnail,\n",
    "        'avgImageZoom': avgImageZoom,\n",
    "        'avgWatchVideo': avgWatchVideo,\n",
    "        'avgView360': avgView360,\n",
    "        'device[mobile]': mobile,\n",
    "        'device[desktop]': desktop,\n",
    "        'device[android]': android,\n",
    "        'device[ios]': ios\n",
    "    },index=[0])\n",
    "\n",
    "def aggrUsers(df):\n",
    "    aggr = df.groupby('customerId').apply(_aggrUser)\n",
    "    aggr.reset_index(inplace=True)\n",
    "    del aggr['customerId']\n",
    "    del aggr['level_1']\n",
    "    return aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filterNanReturnTimeUsers(X, y):\n",
    "    nanTrain = y[y.isnull()].index\n",
    "\n",
    "    return X[~X.customerId.isin(nanTrain)], y[~y.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAggrSet(replace_nans=True):\n",
    "    if replace_nans:\n",
    "        X = pd.read_pickle('../../data/usersAggrNonanFebToFebXDF.pkl')\n",
    "        y = pd.read_pickle('../../data/usersAggrNonanFebToFebYDF.pkl')\n",
    "        return X, y\n",
    "    else:\n",
    "        X = pd.read_pickle('../../data/usersAggrFebToFebXDF.pkl')\n",
    "        y = pd.read_pickle('../../data/usersAggrFebToFebYDF.pkl')\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitAndNormaliseAggr(X, y, test_size=0.33, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    ss = StandardScaler()\n",
    "    X_train_s = ss.fit_transform(X_train)\n",
    "    X_test_s = ss.fit_transform(X_test)\n",
    "    \n",
    "    return X_train_s, X_test_s, y_train, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
