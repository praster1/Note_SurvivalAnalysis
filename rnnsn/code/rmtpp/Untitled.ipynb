{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bayes_opt.bayesian_optimization.BayesianOptimization'>\n",
      "<bayes_opt.bayesian_optimization.BayesianOptimization object at 0x0000020AA0DFBE48>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('C:/Users/JaeseongYoo/Desktop/rnnsm-master/models/rmtpp_baseline.model', 'rb')\n",
    "type(f)\n",
    "#d = pickle.load(f)\n",
    "print(type(d))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy as pt\n",
    "import os.path\n",
    "import pickle\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "obsPeriod = {\n",
    "    'start': pd.Timestamp('2015-02-01'),\n",
    "    'end': pd.Timestamp('2016-02-01')\n",
    "}\n",
    "\n",
    "actPeriod = {\n",
    "    'start': pd.Timestamp('2015-10-01'),\n",
    "    'end': pd.Timestamp('2016-02-01')\n",
    "}\n",
    "\n",
    "predPeriod = {\n",
    "    'start': pd.Timestamp('2016-02-01'),\n",
    "    'end': pd.Timestamp('2016-06-01')\n",
    "}\n",
    "\n",
    "\n",
    "class RmtppData:\n",
    "    PATH = '../../data/rmtpp/'\n",
    "\n",
    "    def instance():\n",
    "        if os.path.isfile(RmtppData.PATH+'rmtpp_data.pkl'):\n",
    "            return pickle.load(open(RmtppData.PATH+'rmtpp_data.pkl', 'rb'))\n",
    "        else:\n",
    "            d = RmtppData()\n",
    "            d._initialise()\n",
    "            return d\n",
    "\n",
    "    def get_xy(self, min_n_sessions=10, n_sessions=10, target_sequences=False, preset='startUserTimeHours'):\n",
    "        x_train, x_test, y_train, y_test = self.x_train, self.x_test, self.y_train_unscaled, self.y_test_unscaled\n",
    "        x_train_unscaled, x_test_unscaled = self.x_train_unscaled, self.x_test_unscaled\n",
    "        feature_indices = self.presets[preset]['feature_indices']\n",
    "        features = self.presets[preset]['features']\n",
    "        target_indices = self.presets[preset]['target_indices']\n",
    "        targets = self.presets[preset]['target']\n",
    "\n",
    "        # if encode_devices:\n",
    "        #     feature_indices = [self.deviceEncIndex] + feature_indices\n",
    "        #     features = ['device'] + features\n",
    "        # else:\n",
    "        #     feature_indices = self.deviceIndices + feature_indices\n",
    "        #     features = self.devices + features\n",
    "\n",
    "        y_train = y_train.apply(lambda x: x.T[target_indices].T)\n",
    "        y_test = y_test.apply(lambda x: x.T[target_indices].T)\n",
    "        x_train = x_train.apply(lambda x: x.T[feature_indices].T)\n",
    "        x_test = x_test.apply(lambda x: x.T[feature_indices].T)\n",
    "        x_train_unscaled = x_train_unscaled.apply(lambda x: x.T[feature_indices].T)\n",
    "        x_test_unscaled = x_test_unscaled.apply(lambda x: x.T[feature_indices].T)\n",
    "\n",
    "        # if not include_churned:\n",
    "        #     x_train = x_train[~x_train.index.isin(self.churned_cust)]\n",
    "        #     # x_test = x_test[~x_test.index.isin(self.churned_cust)]\n",
    "        #     x_train_unscaled = x_train_unscaled[~x_train_unscaled.index.isin(self.churned_cust)]\n",
    "        #     # x_test_unscaled = x_test_unscaled[~x_test_unscaled.index.isin(self.churned_cust)]\n",
    "        #     y_train = y_train[~y_train.index.isin(self.churned_cust)]\n",
    "        #     # y_test = y_test[~y_test.index.isin(self.churned_cust)]\n",
    "\n",
    "        if min_n_sessions > 1:\n",
    "            cust = self.num_sessions[self.num_sessions > min_n_sessions].index\n",
    "            x_train = x_train[x_train.index.isin(cust)]\n",
    "            x_test = x_test[x_test.index.isin(cust)]\n",
    "            x_train_unscaled = x_train_unscaled[x_train_unscaled.index.isin(cust)]\n",
    "            x_test_unscaled = x_test_unscaled[x_test_unscaled.index.isin(cust)]\n",
    "            y_train = y_train[y_train.index.isin(cust)]\n",
    "            y_test = y_test[y_test.index.isin(cust)]\n",
    "\n",
    "        if n_sessions == -1:\n",
    "            n_sessions = self.num_sessions.max()\n",
    "\n",
    "        if target_sequences:\n",
    "            y_train = _pad_x(y_train, n_sessions)\n",
    "            y_test = _pad_x(y_test, n_sessions)\n",
    "        else:\n",
    "            y_train = np.array(y_train.apply(lambda x: x[-1]).tolist())\n",
    "            y_test = np.array(y_test.apply(lambda x: x[-1]).tolist())\n",
    "\n",
    "        x_train = _pad_x(x_train, n_sessions)\n",
    "        x_test = _pad_x(x_test, n_sessions)\n",
    "        x_train_unscaled = _pad_x(x_train_unscaled, n_sessions)\n",
    "        x_test_unscaled = _pad_x(x_test_unscaled, n_sessions)\n",
    "\n",
    "        return x_train, x_test, x_train_unscaled, x_test_unscaled, y_train, y_test, features, targets\n",
    "\n",
    "    def _initialise(self):\n",
    "        df_0 = self.df_0 = pd.read_pickle('../../data/rnn/first/rnn_stage2_df.pkl')\n",
    "        churned = self.churned = df_0.groupby('customerId').last().churned\n",
    "        self.churned_cust = churned.index[churned].values\n",
    "        self.num_sessions = self.df_0.groupby('customerId').customerId.count()\n",
    "\n",
    "        # encoded features (in range 1-..)\n",
    "        self.deviceEncoder = LabelEncoder()\n",
    "        df_0['device_enc'] = self.deviceEncoder.fit_transform(df_0.device) + 1\n",
    "        df_0['hourOfDay_enc'] = df_0.hourOfDay + 1\n",
    "        df_0['dayOfMonth_enc'] = df_0.dayOfMonth\n",
    "        df_0['dayOfWeek_enc'] = df_0.dayOfWeek + 1\n",
    "\n",
    "        # get times in days\n",
    "        df_0['startUserDate'] = df_0.startUserTime.dt.date.apply(pd.Timestamp)\n",
    "        df_0['startUserTimeDays'] = (df_0.startUserDate - obsPeriod['start']) / np.timedelta64(24, 'h')\n",
    "        df_0['deltaNextDays'] = df_0.deltaNextHours / 24\n",
    "        df_0['deltaPrevDays'] = df_0.deltaPrevHours / 24\n",
    "        df_0['logDeltaNextDays'] = np.log(df_0.deltaNextDays + 1)\n",
    "        df_0['logDeltaPrevDays'] = np.log(df_0.deltaPrevDays + 1)\n",
    "\n",
    "        # add nextStartUserTimeHours\n",
    "        df_0['nextStartUserTimeHours'] = df_0.startUserTimeHours + df_0.deltaNextHours\n",
    "        df_0['nextStartUserTimeDays'] = df_0.startUserTimeDays + df_0.deltaNextDays\n",
    "\n",
    "        # train/test split, stratify by churn\n",
    "        train_i, test_i = self.train_i, self.test_i = next(StratifiedShuffleSplit(test_size=.2, random_state=42).split(churned, churned.values))\n",
    "        train_df_unscaled = self.train_df_unscaled = df_0[df_0.customerId.isin(churned.index[train_i])]\n",
    "        test_df_unscaled = self.test_df_unscaled = df_0[df_0.customerId.isin(churned.index[test_i])]\n",
    "\n",
    "        train_features = self.train_features = sorted(list(set(df_0.columns) - set(['customerId','startUserTime', 'startUserDate'])))\n",
    "        target_features = self.target_features = ['nextStartUserTimeDays', 'deltaNextDays', 'nextStartUserTimeHours', 'deltaNextHours', 'churned', 'logDeltaNextDays']\n",
    "        enc_features = [f for f in train_features if f.endswith('_enc')]\n",
    "        self.deviceEncIndex = train_features.index('device_enc')\n",
    "        self.devices = [x for x in train_features if x.startswith('device[')]\n",
    "        self.deviceIndices = list(map(train_features.index, self.devices))\n",
    "\n",
    "        # scaling\n",
    "        features_numeric = self.features_numeric = sorted(list(set(df_0.columns) - set(['customerId','startUserTime', 'startUserDate', 'device_enc', 'device', 'hourOfDay_enc', 'dayOfMonth_enc', 'dayOfWeek_enc'] + self.devices)))\n",
    "        train_df_scaled = self.train_df_scaled = train_df_unscaled.copy()\n",
    "        test_df_scaled = test_df_unscaled.copy()\n",
    "        scaler = self.scaler = StandardScaler()\n",
    "        train_df_scaled[features_numeric] = scaler.fit_transform(train_df_unscaled[features_numeric])\n",
    "        test_df_scaled[features_numeric] = scaler.transform(test_df_unscaled[features_numeric])\n",
    "        self.train_df_scaled = train_df_scaled\n",
    "        self.test_df_scaled = test_df_scaled\n",
    "\n",
    "        # storing feature/target combinations as features for quick access\n",
    "        # format: predict/mainFeature\n",
    "        self.presets = {\n",
    "            'deltaNextDays': {\n",
    "                'features': sorted(list(set(self.train_features) - \\\n",
    "                                        set(['deltaNextHours', 'startUserTimeHours',\n",
    "                                             'deltaNextDays', 'deltaPrevHours', 'churned',\n",
    "                                             'device_enc', 'device', 'nextStartUserTimeHours',\n",
    "                                             'nextStartUserTimeDays',\n",
    "                                             'logDeltaNextDays', 'logDeltaPrevDays'] + enc_features))),\n",
    "                'target': ['nextStartUserTimeDays', 'deltaNextDays', 'churned'] },\n",
    "\n",
    "            'deltaNextDays_enc': {\n",
    "                'features': sorted(list(set(self.train_features) - \\\n",
    "                                        set(['deltaNextHours', 'startUserTimeHours',\n",
    "                                             'deltaNextDays', 'deltaPrevHours', 'churned',\n",
    "                                             'device', 'nextStartUserTimeHours',\n",
    "                                             'nextStartUserTimeDays', 'hourOfDay', 'dayOfWeek',\n",
    "                                             'dayOfMonth', 'logDeltaNextDays',\n",
    "                                             'logDeltaPrevDays'] + self.devices))),\n",
    "                'target': ['nextStartUserTimeDays', 'deltaNextDays', 'churned'] },\n",
    "\n",
    "            'logDeltaNextDays_enc': {\n",
    "                'features': sorted(list(set(self.train_features) - \\\n",
    "                                        set(['deltaNextHours', 'startUserTimeHours',\n",
    "                                             'deltaNextDays', 'deltaPrevHours', 'churned',\n",
    "                                             'device', 'nextStartUserTimeHours',\n",
    "                                             'nextStartUserTimeDays', 'hourOfDay', 'dayOfWeek',\n",
    "                                             'dayOfMonth', 'logDeltaNextDays',\n",
    "                                             'deltaPrevDays'] + self.devices))),\n",
    "                'target': ['nextStartUserTimeDays', 'logDeltaNextDays', 'churned'] },\n",
    "\n",
    "            'nextStartUserTimeDays': {\n",
    "                'features': sorted(list(set(self.train_features) - \\\n",
    "                                        set(['deltaNextHours', 'startUserTimeHours',\n",
    "                                             'deltaNextDays', 'deltaPrevHours', 'churned',\n",
    "                                             'device_enc', 'device', 'nextStartUserTimeHours',\n",
    "                                             'nextStartUserTimeDays',\n",
    "                                             'logDeltaNextDays', 'logDeltaPrevDays'] + enc_features))),\n",
    "                'target': ['nextStartUserTimeDays', 'deltaNextDays', 'churned'] },\n",
    "\n",
    "            'nextStartUserTimeDays_enc': {\n",
    "                'features': sorted(list(set(self.train_features) - \\\n",
    "                                        set(['deltaNextHours', 'startUserTimeHours',\n",
    "                                             'deltaNextDays', 'deltaPrevHours', 'churned',\n",
    "                                             'device', 'nextStartUserTimeHours',\n",
    "                                             'nextStartUserTimeDays', 'hourOfDay', 'dayOfWeek',\n",
    "                                             'dayOfMonth', 'logDeltaNextDays',\n",
    "                                             'logDeltaPrevDays'] + self.devices))),\n",
    "                'target': ['nextStartUserTimeDays', 'deltaNextDays', 'churned'] }}\n",
    "\n",
    "        for preset in self.presets:\n",
    "            self.presets[preset]['feature_indices'] = list(map(self.train_features.index, self.presets[preset]['features']))\n",
    "            self.presets[preset]['target_indices'] = list(map(self.target_features.index, self.presets[preset]['target']))\n",
    "\n",
    "        # convert to array\n",
    "        self.x_train, self.y_train = _df_to_xy_array(train_df_scaled, train_features, target_features)\n",
    "        self.x_train_unscaled, self.y_train_unscaled = _df_to_xy_array(train_df_unscaled, train_features, target_features)\n",
    "        self.x_test, self.y_test = _df_to_xy_array(test_df_scaled, train_features, target_features)\n",
    "        self.x_test_unscaled, self.y_test_unscaled = _df_to_xy_array(test_df_unscaled, train_features, target_features)\n",
    "\n",
    "        # store customer ids\n",
    "        train_cust = self.train_cust = self.y_train.index.values\n",
    "        test_cust = self.test_cust = self.y_test.index.values\n",
    "\n",
    "        with open(self.PATH+'rmtpp_data.pkl', 'wb') as handle:\n",
    "            pickle.dump(self, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def _pad_x(x, n):\n",
    "    return np.array(list(map(lambda _x: np.pad(_x[-n:], ((max(n-len(_x),0),0),(0,0)), 'constant'), x.values)))\n",
    "\n",
    "def _pad_y(x, n):\n",
    "    return np.array(list(map(lambda _x: np.pad(_x[-n:], (max(n-len(_x),0), 0), 'constant'), x.values)))\n",
    "\n",
    "def _df_to_xy_array(df, train_features, target_features):\n",
    "    grouped = df.groupby('customerId')\n",
    "    x = grouped.apply(lambda g: g[train_features].as_matrix())\n",
    "    y = grouped.apply(lambda g: g[target_features].as_matrix())\n",
    "\n",
    "    return x, y\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
